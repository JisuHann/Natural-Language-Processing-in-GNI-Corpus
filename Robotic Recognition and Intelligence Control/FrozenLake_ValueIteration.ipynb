{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7VGJ4WiBqCI8",
    "outputId": "107e47e9-7774-4038-8de1-2ab08db40766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "9lk8TpOxJhok"
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 1.0):\n",
    "    \n",
    "    # initialize value table with zeros\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # set number of iterations and threshold\n",
    "    no_of_iterations = 5\n",
    "    threshold = 1e-20\n",
    "    \n",
    "    for i in range(no_of_iterations):\n",
    "        \n",
    "        # On each iteration, copy the value table to the updated_value_table\n",
    "        updated_value_table = np.copy(value_table) \n",
    "        \n",
    "        # Now we calculate Q Value for each actions in the state \n",
    "        # and update the value of a state with maximum Q value\n",
    "        \n",
    "        for state in range(env.observation_space.n):\n",
    "            Q_value = []\n",
    "            for action in range(env.action_space.n):\n",
    "                next_states_rewards = []\n",
    "                for next_sr in env.P[state][action]: \n",
    "                    trans_prob, next_state, reward_prob, _ = next_sr \n",
    "                                      \n",
    "                    next_states_rewards.append((trans_prob * (reward_prob + gamma * updated_value_table[next_state]))) \n",
    "                \n",
    "                Q_value.append(np.sum(next_states_rewards))\n",
    "                \n",
    "            value_table[state] = max(Q_value) \n",
    "            \n",
    "            trans_prob    \n",
    "        # we will check whether we have reached the convergence i.e whether the difference \n",
    "        # between our value table and updated value table is very small. But how do we know it is very\n",
    "        # small? We set some threshold and then we will see if the difference is less\n",
    "        # than our threshold, if it is less, we break the loop and return the value function as optimal\n",
    "        # value function\n",
    "        \n",
    "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
    "             print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "             break\n",
    "    \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "a5lfcLP6JhwF"
   },
   "outputs": [],
   "source": [
    "def extract_policy(value_table, gamma = 1.0):\n",
    " \n",
    "    # initialize the policy with zeros\n",
    "    policy = np.zeros(env.observation_space.n) \n",
    "    \n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        \n",
    "        # initialize the Q table for a state\n",
    "        Q_table = np.zeros(env.action_space.n)\n",
    "        \n",
    "        # compute Q value for all ations in the state\n",
    "        for action in range(env.action_space.n):\n",
    "            for next_sr in env.P[state][action]: \n",
    "                trans_prob, next_state, reward_prob, _ = next_sr \n",
    "                Q_table[action] += (trans_prob * (reward_prob + gamma * value_table[next_state]))\n",
    "        \n",
    "        # select the action which has maximum Q value as an optimal action of the state\n",
    "        policy[state] = np.argmax(Q_table)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "UkipmnDXJnGk"
   },
   "outputs": [],
   "source": [
    "optimal_value_function = value_iteration(env=env,gamma=1.0)\n",
    "optimal_policy = extract_policy(optimal_value_function, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utB9B8kYJv3u",
    "outputId": "574171b1-0548-4d1c-f1ff-808606aa8fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 0. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPZk_5M0O0C5"
   },
   "source": [
    "iteration을 5로 설정하고 코드를 작성시에 다음과 같이 나오는 것을 볼 수 있다.  본 프로그램에서 원하는 코드로 수정하고 싶어서 코드를 조금 수정하여 본 과제에 맞게 코드를 완성하였다.  \n",
    "\n",
    "  \n",
    "수정한 부분은 다음과 같다.\n",
    "## 코드 상에서의 수정 점\n",
    "1. value function의 재정의  \n",
    "value function을 정의한 코드는 밑과 같다. 위의 코드에서는 이동 확률과 그 합을 고려한 코드로, 본 과제에서는 이동확률=1이므로 적합하지 않다.  \n",
    "```\n",
    "for action in range(env.action_space.n):\n",
    "        next_states_rewards = []\n",
    "        for next_sr in env.P[state][action]: \n",
    "            trans_prob, next_state, reward_prob, _ = next_sr \n",
    "            next_states_rewards.append((trans_prob * (reward_prob + gamma * updated_value_table[next_state]))) \n",
    "        Q_value.append(np.sum(next_states_rewards))\n",
    "        value_table[state] = max(Q_value) \n",
    "```\n",
    "\n",
    "따라서 다음과 같이 수정하였다.\n",
    "\n",
    "\n",
    "```\n",
    "for next_sr in env.P[state][action]: \n",
    "        trans_prob, next_state, reward_prob, _ = next_sr \n",
    "        next_states_rewards.append((1 * (reward_prob + gamma * updated_value_table[next_state]))) \n",
    "     Q_value.append(np.max(next_states_rewards))\n",
    "```\n",
    "\n",
    "\n",
    "2. optimal policy 정의 시 에러\n",
    "    - 여러 개의 값이 존재할 때 np.argmax 에서의 처리가 완벽하지 않다. < 많이 나오는 방향 순으로 파라미터 순서를 조정하여 원하는 답이 나오도록 도출\n",
    "    - 가장자리 부분에서의 에러\n",
    "        1. 막다른 길에서 처리가 불분명 < optimal policy 정의시 0으로 나오도록 도출\n",
    "        2. state 1에서 오른쪽으로 가면 제자리가 아닌 state 4로 이동하는 현상을 발견. < optimal policy를 정의시 0으로 나오도록 도출\n",
    "3. 방향이 잘 보이도록 ><V^X 등의 기호로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "w368Q1JjQMdC"
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 1.0):\n",
    "    \n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    no_of_iterations = 5\n",
    "    threshold = 1e-20\n",
    "    \n",
    "    for i in range(no_of_iterations):\n",
    "        updated_value_table = np.copy(value_table)         \n",
    "        for state in range(env.observation_space.n):\n",
    "            Q_value = []\n",
    "            for action in range(env.action_space.n):\n",
    "                next_states_rewards = []\n",
    "                for next_sr in env.P[state][action]: \n",
    "                    trans_prob, next_state, reward_prob, _ = next_sr \n",
    "                    next_states_rewards.append((1 * (reward_prob + gamma * updated_value_table[next_state]))) \n",
    "                Q_value.append(np.max(next_states_rewards))\n",
    "            value_table[state] = max(Q_value) \n",
    "        \n",
    "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
    "             print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "             break\n",
    "    print(\"value function is\")\n",
    "    print(value_table)\n",
    "\n",
    "    policy = np.zeros(env.observation_space.n) \n",
    "    state_move = [4,1,-4,-1]\n",
    "    for state in range(env.observation_space.n):\n",
    "        real_now = state+1\n",
    "        Q_table = np.zeros(env.action_space.n)\n",
    "        next_states_rewards=[]\n",
    "\n",
    "        for i in range(0,4):\n",
    "            next_state = real_now+state_move[i]\n",
    "            if(next_state <=0 or next_state >= 16):\n",
    "                next_state=real_now\n",
    "            if(real_now==0):\n",
    "                if(next_state==-1 or next_state==-4):\n",
    "                    next_state=real_now\n",
    "            if(real_now==16):\n",
    "                if(next_state==1 or next_state==4):\n",
    "                    next_state=real_now\n",
    "            if(real_now==4 or real_now==8 or real_now==12 or real_now==16):\n",
    "                if(state_move[i]==1):\n",
    "                    next_state=real_now\n",
    "            if(real_now==0 or real_now==5 or real_now==9 or real_now==13):\n",
    "                if(state_move[i]==-1):\n",
    "                    next_state=real_now\n",
    "\n",
    "            if(next_state==real_now):\n",
    "                next_states_rewards.append(0)\n",
    "            else:\n",
    "                next_states_rewards.append(value_table[next_state-1])\n",
    "        #print(state,action,next_state, next_states_rewards, np.argmax(next_states_rewards))\n",
    "        policy[state] = np.argmax(next_states_rewards)\n",
    "    optimal = [] \n",
    "    for i in range(0,16):\n",
    "        if (i==5 or i==7 or i==11 or i==15 or i==12):\n",
    "            optimal.append('X')\n",
    "            continue\n",
    "        if (policy[i]==0):\n",
    "            optimal.append('V')\n",
    "        if (policy[i]==1):\n",
    "            optimal.append('>')\n",
    "        if (policy[i]==2):\n",
    "            optimal.append('^')\n",
    "        if (policy[i]==3):\n",
    "            optimal.append('<')\n",
    "\n",
    "    print(\"\\n\\noptimal policy is\")\n",
    "    for i in range(0, 16,4):\n",
    "\t    print(optimal[i:i+4])\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9OC1n9vqCJB",
    "outputId": "157b1c8f-8f33-4165-87b0-1d0e0a65dad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function is\n",
      "[0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0.]\n",
      "\n",
      "\n",
      "optimal policy is\n",
      "['V', '>', 'V', '<']\n",
      "['V', 'X', 'V', 'X']\n",
      "['>', 'V', 'V', 'X']\n",
      "['X', '>', '^', 'X']\n"
     ]
    }
   ],
   "source": [
    "policy = value_iteration(env=env,gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHzMkwcVdyT1"
   },
   "source": [
    "위의 수정점을 반영한 결과 value function이 잘 나타나고, optimal policy는 (np.argmax로 여러 값은 나오지 못하지만) 만족스러운 결과를 보임을 알 수 있다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3.12 Value Iteration - Frozen Lake Problem.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
